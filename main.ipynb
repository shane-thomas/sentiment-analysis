{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ec7a44-9534-4f58-87ae-5e91693ffc20",
   "metadata": {},
   "source": [
    "Neural Network for sentiment Analysis multi class labelling\n",
    "Import libraries necessary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb04b5-048d-40c4-83d2-6bec2db3eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c2e9b-dec0-46e9-a90a-55efff53f075",
   "metadata": {},
   "source": [
    "Dataset pre-processing:\n",
    "1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a15be-35eb-475d-9da4-2b512a9ff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': 'Reviews.csv'}, delimiter=',')\n",
    "\n",
    "def preprocess(example):\n",
    "    text = (example['Summary'] or \"\") + \": \" + (example['Text'] or \"\")\n",
    "    return {\n",
    "        'Text': text,\n",
    "        'label': int(example['Score']) - 1  #converting the examples from 1-5 to 0-4\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d9c7b",
   "metadata": {},
   "source": [
    "2. Cast Class Labels to columns (necessary for working with datasets library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e52c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_feature = ClassLabel(num_classes=5, names=[\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"])\n",
    "dataset = dataset.cast_column('label', label_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed06c1f",
   "metadata": {},
   "source": [
    "3. Splitting the dataset into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1101790",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, _ = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.75,  # Keep only 25% of the training data\n",
    "    stratify_by_column='label', seed=42).values()\n",
    "train_split, temp_split = train_split.train_test_split(test_size=0.2, stratify_by_column='label').values()\n",
    "val_split, test_split = temp_split.train_test_split(test_size=0.5, stratify_by_column='label').values()\n",
    "\n",
    "print(\"\\nLabel distribution in training subset:\")\n",
    "for label in range(5):  \n",
    "    count = sum(1 for l in train_split['label'] if l == label)\n",
    "    print(f\"  Label {label}: {count} samples ({count/len(train_split)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0c3ee",
   "metadata": {},
   "source": [
    "4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78208082",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=120,\n",
    "    )\n",
    "\n",
    "train_dataset = train_split.map(tokenize_function, batched=True)\n",
    "val_dataset = val_split.map(tokenize_function, batched=True)\n",
    "test_dataset = test_split.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ba316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print()\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbd862",
   "metadata": {},
   "source": [
    "Adding Data into dataloader batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be56e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=12, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=12, pin_memory=True)\n",
    "print(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff516f0",
   "metadata": {},
   "source": [
    "Defining an embedding extraction function and an embedding dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7981df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_distilbert_embeddings(data_loader, device):\n",
    "    distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    distilbert.to(device)\n",
    "    distilbert.eval()  \n",
    "   \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Extracting Embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"]\n",
    "           \n",
    "            outputs = distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "           \n",
    "            # Use CLS token embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels)\n",
    "   \n",
    "    embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
    "    labels_tensor = torch.cat(all_labels, dim=0)\n",
    "   \n",
    "    return embeddings_tensor, labels_tensor\n",
    "\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"embeddings\": self.embeddings[idx], \"label\": self.labels[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2ebd7",
   "metadata": {},
   "source": [
    "Extract embeddings for the all the splits. \n",
    "Then manipulate them into new dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a44e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Extracting embeddings from training data...\")\n",
    "train_embeddings, train_labels = extract_distilbert_embeddings(train_dataloader, device)\n",
    "print(\"Extracting embeddings from validation data...\")\n",
    "val_embeddings, val_labels = extract_distilbert_embeddings(val_dataloader, device)\n",
    "print(\"Extracting embeddings from test data...\")\n",
    "test_embeddings, test_labels = extract_distilbert_embeddings(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8a054",
   "metadata": {},
   "source": [
    "Create the datasets that we mentioned before using custom class and then convert them into dataloaders\n",
    "The reason we need an intermediate dataset class is that PyTorch's DataLoader requires a dataset object that implements the __len__ and __getitem__ methods as its first argument. It can't work directly with raw tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d54673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_emb_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
    "val_emb_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
    "test_emb_dataset = EmbeddingDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "emb_batch_size = 128 \n",
    "train_emb_dataloader = torch.utils.data.DataLoader(train_emb_dataset, batch_size=emb_batch_size, shuffle=True)\n",
    "val_emb_dataloader = torch.utils.data.DataLoader(val_emb_dataset, batch_size=emb_batch_size)\n",
    "test_emb_dataloader = torch.utils.data.DataLoader(test_emb_dataset, batch_size=emb_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edd5b8",
   "metadata": {},
   "source": [
    "Training your own Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_labels=5):  # DistilBERT embeddings are 768 dimensions\n",
    "        super(Sentiment_Model, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "model = Sentiment_Model(input_dim=768, num_labels=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dab07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_emb_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", \n",
    "                       bar_format='{l_bar}{bar:30}{r_bar}')\n",
    "    \n",
    "    # Training loop\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        embeddings = batch[\"embeddings\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch accuracy for display\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_loss = total_loss / (step + 1)\n",
    "        batch_acc = correct_train / total_train * 100\n",
    "        \n",
    "        # Update progress bar with TensorFlow-like metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'accuracy': f'{batch_acc:.2f}%',\n",
    "        })\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_emb_dataloader:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            val_loss += loss_fn(outputs, labels).item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_emb_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_emb_dataloader)\n",
    "    train_acc = correct_train / total_train * 100\n",
    "    val_acc = correct_val / total_val * 100\n",
    "    \n",
    "    # Print TensorFlow-style epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \" \n",
    "          f\"loss: {avg_train_loss:.4f} - \"\n",
    "          f\"accuracy: {train_acc:.2f}% - \"\n",
    "          f\"val_loss: {avg_val_loss:.4f} - \"\n",
    "          f\"val_accuracy: {val_acc:.2f}% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04654ba7",
   "metadata": {},
   "source": [
    "Evaluation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-point tolerance evaluation\n",
    "def relaxed_accuracy(preds, labels, tolerance=1):\n",
    "    preds = torch.tensor(preds)\n",
    "    labels = torch.tensor(labels)\n",
    "    return ((preds - labels).abs() <= tolerance).float().mean().item()\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_emb_dataloader:\n",
    "        embeddings = batch[\"embeddings\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute relaxed accuracy\n",
    "acc = relaxed_accuracy(all_preds, all_labels, 0)\n",
    "relaxed_acc = relaxed_accuracy(all_preds, all_labels)\n",
    "print(f\"\\nTest Accuracy: {acc * 100:.2f}%\\nRelaxed Test Accuracy (±1): {relaxed_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd681e13",
   "metadata": {},
   "source": [
    "Predicting sentiment value of custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, distilbert_model, classifier_model, device):\n",
    "    distilbert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = classifier_model(embedding)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = input(\"Enter text: \")\n",
    "predicted_class = predict_sentiment(input_text, distilbert_model, model, device)\n",
    "category = {0:\"negative\", 1: \"somewhat negative\", 2: \"neutral\", 3:\"somewhat positive\", 4:\"positive\"}\n",
    "print(f\"Predicted Sentiment Class: {predicted_class+1} \\n{category[predicted_class]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
