{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ec7a44-9534-4f58-87ae-5e91693ffc20",
   "metadata": {},
   "source": [
    "Neural Network for sentiment Analysis multi class labelling\n",
    "Import libraries necessary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cb04b5-048d-40c4-83d2-6bec2db3eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c2e9b-dec0-46e9-a90a-55efff53f075",
   "metadata": {},
   "source": [
    "Dataset pre-processing:\n",
    "1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077a15be-35eb-475d-9da4-2b512a9ff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': 'Reviews.csv'}, delimiter=',')\n",
    "\n",
    "def preprocess(example):\n",
    "    text = (example['Summary'] or \"\") + \": \" + (example['Text'] or \"\")\n",
    "    return {\n",
    "        'Text': text,\n",
    "        'label': int(example['Score']) - 1  #converting the examples from 1-5 to 0-4\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d9c7b",
   "metadata": {},
   "source": [
    "2. Cast Class Labels to columns (necessary for working with datasets library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e52c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_feature = ClassLabel(num_classes=5, names=[\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"])\n",
    "dataset = dataset.cast_column('label', label_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed06c1f",
   "metadata": {},
   "source": [
    "3. Splitting the dataset into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948e8f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Text', 'label'],\n",
      "        num_rows: 568454\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1101790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in training subset:\n",
      "  Label 0: 10453 samples (9.19%)\n",
      "  Label 1: 5954 samples (5.24%)\n",
      "  Label 2: 8528 samples (7.50%)\n",
      "  Label 3: 16131 samples (14.19%)\n",
      "  Label 4: 72624 samples (63.88%)\n"
     ]
    }
   ],
   "source": [
    "train_split, _ = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.75,  # Keep only 25% of the training data\n",
    "    stratify_by_column='label', seed=42).values()\n",
    "train_split, temp_split = train_split.train_test_split(test_size=0.2, stratify_by_column='label').values()\n",
    "val_split, test_split = temp_split.train_test_split(test_size=0.5, stratify_by_column='label').values()\n",
    "\n",
    "print(\"\\nLabel distribution in training subset:\")\n",
    "for label in range(5):  \n",
    "    count = sum(1 for l in train_split['label'] if l == label)\n",
    "    print(f\"  Label {label}: {count} samples ({count/len(train_split)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0c3ee",
   "metadata": {},
   "source": [
    "4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78208082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d969ebc6084b3799d653aaa77d16aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113690 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260de813125f4baabca7f8f83609716b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70337be0338c42c784edbf1fffaebd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=120,\n",
    "    )\n",
    "\n",
    "train_dataset = train_split.map(tokenize_function, batched=True)\n",
    "val_dataset = val_split.map(tokenize_function, batched=True)\n",
    "test_dataset = test_split.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02ba316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 113690\n",
      "})\n",
      "\n",
      "{'label': tensor(4), 'input_ids': tensor([  101,  3811, 16755,  1024,  2200, 11937, 21756,  1998,  2440,  1997,\n",
      "        11917,  1012,  1037,  2200, 17087,  1010, 24514,  2100, 19782,  2000,\n",
      "         2131,  2017,  2083,  1011,  2008,  2017,  2064,  2514,  2204,  2055,\n",
      "         5983,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print()\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbd862",
   "metadata": {},
   "source": [
    "Adding Data into dataloader batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be56e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 113690\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=12, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=12, pin_memory=True)\n",
    "print(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff516f0",
   "metadata": {},
   "source": [
    "Defining an embedding extraction function and an embedding dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7981df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_distilbert_embeddings(data_loader, device):\n",
    "    distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    distilbert.to(device)\n",
    "    distilbert.eval()  \n",
    "   \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Extracting Embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"]\n",
    "           \n",
    "            outputs = distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "           \n",
    "            # Use CLS token embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels)\n",
    "   \n",
    "    embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
    "    labels_tensor = torch.cat(all_labels, dim=0)\n",
    "   \n",
    "    return embeddings_tensor, labels_tensor\n",
    "\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"embeddings\": self.embeddings[idx], \"label\": self.labels[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2ebd7",
   "metadata": {},
   "source": [
    "Extract embeddings for the all the splits. \n",
    "Then manipulate them into new dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a44e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 1777/1777 [07:27<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 223/223 [01:04<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings from test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 223/223 [01:05<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Extracting embeddings from training data...\")\n",
    "train_embeddings, train_labels = extract_distilbert_embeddings(train_dataloader, device)\n",
    "print(\"Extracting embeddings from validation data...\")\n",
    "val_embeddings, val_labels = extract_distilbert_embeddings(val_dataloader, device)\n",
    "print(\"Extracting embeddings from test data...\")\n",
    "test_embeddings, test_labels = extract_distilbert_embeddings(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8a054",
   "metadata": {},
   "source": [
    "Create the datasets that we mentioned before using custom class and then convert them into dataloaders\n",
    "The reason we need an intermediate dataset class is that PyTorch's DataLoader requires a dataset object that implements the __len__ and __getitem__ methods as its first argument. It can't work directly with raw tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d54673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_emb_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
    "val_emb_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
    "test_emb_dataset = EmbeddingDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "emb_batch_size = 128 \n",
    "train_emb_dataloader = torch.utils.data.DataLoader(train_emb_dataset, batch_size=emb_batch_size, shuffle=True)\n",
    "val_emb_dataloader = torch.utils.data.DataLoader(val_emb_dataset, batch_size=emb_batch_size)\n",
    "test_emb_dataloader = torch.utils.data.DataLoader(test_emb_dataset, batch_size=emb_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edd5b8",
   "metadata": {},
   "source": [
    "Training your own Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c733c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Model(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_labels=5):  # DistilBERT embeddings are 768 dimensions\n",
    "        super(Sentiment_Model, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "model = Sentiment_Model(input_dim=768, num_labels=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7dab07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████████████| 889/889 [00:04<00:00, 180.18it/s, loss=0.8688, accuracy=68.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - loss: 0.8688 - accuracy: 68.23% - val_loss: 0.7866 - val_accuracy: 70.73% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 172.26it/s, loss=0.7735, accuracy=70.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - loss: 0.7735 - accuracy: 70.91% - val_loss: 0.7609 - val_accuracy: 71.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████████████| 889/889 [00:04<00:00, 178.46it/s, loss=0.7563, accuracy=71.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - loss: 0.7563 - accuracy: 71.50% - val_loss: 0.7494 - val_accuracy: 71.71% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 170.47it/s, loss=0.7452, accuracy=71.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - loss: 0.7452 - accuracy: 71.72% - val_loss: 0.7548 - val_accuracy: 71.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 163.96it/s, loss=0.7392, accuracy=72.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - loss: 0.7392 - accuracy: 72.02% - val_loss: 0.7502 - val_accuracy: 71.66% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 169.12it/s, loss=0.7317, accuracy=72.27%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - loss: 0.7317 - accuracy: 72.27% - val_loss: 0.7304 - val_accuracy: 72.39% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 163.08it/s, loss=0.7266, accuracy=72.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - loss: 0.7266 - accuracy: 72.39% - val_loss: 0.7304 - val_accuracy: 72.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 170.93it/s, loss=0.7223, accuracy=72.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - loss: 0.7223 - accuracy: 72.58% - val_loss: 0.7228 - val_accuracy: 72.63% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 166.69it/s, loss=0.7178, accuracy=72.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - loss: 0.7178 - accuracy: 72.67% - val_loss: 0.7202 - val_accuracy: 72.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████████| 889/889 [00:05<00:00, 168.07it/s, loss=0.7140, accuracy=72.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - loss: 0.7140 - accuracy: 72.85% - val_loss: 0.7179 - val_accuracy: 72.73% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_emb_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", \n",
    "                       bar_format='{l_bar}{bar:30}{r_bar}')\n",
    "    \n",
    "    # Training loop\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        embeddings = batch[\"embeddings\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch accuracy for display\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_loss = total_loss / (step + 1)\n",
    "        batch_acc = correct_train / total_train * 100\n",
    "        \n",
    "        # Update progress bar with TensorFlow-like metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'accuracy': f'{batch_acc:.2f}%',\n",
    "        })\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_emb_dataloader:\n",
    "            embeddings = batch[\"embeddings\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            val_loss += loss_fn(outputs, labels).item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_emb_dataloader)\n",
    "    avg_val_loss = val_loss / len(val_emb_dataloader)\n",
    "    train_acc = correct_train / total_train * 100\n",
    "    val_acc = correct_val / total_val * 100\n",
    "    \n",
    "    # Print TensorFlow-style epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \" \n",
    "          f\"loss: {avg_train_loss:.4f} - \"\n",
    "          f\"accuracy: {train_acc:.2f}% - \"\n",
    "          f\"val_loss: {avg_val_loss:.4f} - \"\n",
    "          f\"val_accuracy: {val_acc:.2f}% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04654ba7",
   "metadata": {},
   "source": [
    "Evaluation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ccdbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 72.82%\n",
      "Relaxed Test Accuracy (±1): 90.50%\n"
     ]
    }
   ],
   "source": [
    "# 1-point tolerance evaluation\n",
    "def relaxed_accuracy(preds, labels, tolerance=1):\n",
    "    preds = torch.tensor(preds)\n",
    "    labels = torch.tensor(labels)\n",
    "    return ((preds - labels).abs() <= tolerance).float().mean().item()\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_emb_dataloader:\n",
    "        embeddings = batch[\"embeddings\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute relaxed accuracy\n",
    "acc = relaxed_accuracy(all_preds, all_labels, 0)\n",
    "relaxed_acc = relaxed_accuracy(all_preds, all_labels)\n",
    "print(f\"\\nTest Accuracy: {acc * 100:.2f}%\\nRelaxed Test Accuracy (±1): {relaxed_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd681e13",
   "metadata": {},
   "source": [
    "Predicting sentiment value of custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4401941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, distilbert_model, classifier_model, device):\n",
    "    distilbert_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Pass through the classifier\n",
    "        logits = classifier_model(embedding)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125c10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Class: 5 \n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "input_text = input(\"Enter text: \")\n",
    "predicted_class = predict_sentiment(input_text, distilbert_model, model, device)\n",
    "category = {0:\"negative\", 1: \"somewhat negative\", 2: \"neutral\", 3:\"somewhat positive\", 4:\"positive\"}\n",
    "print(f\"Predicted Sentiment Class: {predicted_class+1} \\n{category[predicted_class]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
